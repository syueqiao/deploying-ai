{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe851e5",
   "metadata": {},
   "source": [
    "# AI as Judge\n",
    "\n",
    "[G-Eval](https://deepeval.com/docs/metrics-llm-evals) is a framework that uses LLM as a judge to evaluate LLM outputs. The evaluation can be based on any criteria. G-Eval is implemented by a library called [DeepEval](https://deepeval.com/) which includes a broader set of tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0650f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../../05_src/.secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c4c3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Find and load the .env file\n",
    "load_dotenv(\"../../05_src/.secrets\")\n",
    "\n",
    "# Verify it works\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "print(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd5ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "document_folder = \"../../05_src/documents/\"\n",
    "blue_cross_file = \"the_blue_cross.txt\"\n",
    "file_path = os.path.join(document_folder, blue_cross_file)\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    blue_cross_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "843da553",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"You are an helpful assistant that summarizes works of fiction with a quirky and bubbly approach.\"\n",
    "PROMPT = \"\"\"\n",
    "    Summarize the following story in at most four paragraphs. Please include all key characters and plot points.\n",
    "    <story>\n",
    "    {story}\n",
    "    </story>\n",
    "    In addition to the summary, add an introduction paragraph where you greet the reader and a conclusion where you share an opinion about the story. Keep the output under 150 words.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f951529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "client = OpenAI(base_url='https://k7uffyg03f.execute-api.us-east-1.amazonaws.com/prod/openai/v1', \n",
    "                api_key='any value',\n",
    "                default_headers={\"x-api-key\": os.getenv('API_GATEWAY_KEY')})\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    instructions=instructions,\n",
    "    input=[\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": PROMPT.format(story=blue_cross_text)}\n",
    "    ],\n",
    "    temperature=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "269743e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Hello, dear reader!** üåº Ready for a delightful romp through a whimsical world of mystery and mischief? Let‚Äôs dive into ‚ÄúThe Blue Cross‚Äù by the brilliant G.K. Chesterton, where crime meets cleverness and one humble priest outwits a notorious criminal. It‚Äôs a tale infused with humor, intrigue, and the unexpected twists that only Father Brown can deliver!\\n\\nIn this tale, we meet the highly intelligent police chief, **Aristide Valentin**, who is hot on the trail of the infamous criminal mastermind, **Flambeau**. Disguised as an unassuming man with a flair for fashion, Valentin is determined to apprehend Flambeau, who evaded capture across multiple countries. The chase begins with Valentin arriving in London, hoping to blend in during the bustling Eucharistic Congress to snare his cunning prey, who constantly changes disguises but has one feature that stands out‚Äîhis towering height.\\n\\nAs the story unfolds, Valentin encounters a clumsy, endearing little priest, **Father Brown**, whose innocent simplicity bemuses the seasoned detective. While Valentin scrutinizes the other passengers on his train, including market gardeners and a short widow, Father Brown remains oblivious, clumsily dropping his belongings while unwittingly becoming a crucial piece in the unfolding mystery. There‚Äôs a delightful interplay of chance and intelligence as Valentin follows a series of bizarre occurrences, including a splash of soup on a wall and a mix-up of sugar and salt! \\n\\nIn a whimsical twist, it is Father Brown who ultimately unveils Flambeau\\'s deceptions. With his insightful observations and clever tactics, he outsmarts the infamous thief by deducing his intentions and ultimately revealing that he had already switched the precious sapphire cross with a decoy. The interaction between Father Brown and Flambeau is both amusing and enlightening, demonstrating that true wisdom often lies in humility and attentiveness. As they cap off their charming escapade, both Valentin and Flambeau bow to the true master of detection, Father Brown‚Äîwhat a twist in the tale! \\n\\n**Oh, what a delightful read!** üåü \"The Blue Cross\" masterfully blends humor, intrigue, and a touch of the philosophical‚Äîshowcasing how often the simplest minds hold the highest wisdom. My heart danced at the clever interplay between characters, reminding us that appearances can be deceiving and that sometimes, the most unassuming person in the room holds the biggest secrets. A whimsical triumph, indeed! üé©‚ú®'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcae503b",
   "metadata": {},
   "source": [
    "# Answer Relevancy\n",
    "\n",
    "The answer relevancy metric evaluates how relevant the actual output of the LLM app is compared to the provided input. This metric is self-explaining in the sense that the output includes a reason for the metric score.\n",
    "\n",
    "The metric is calculated as:\n",
    "\n",
    "$$\n",
    "AnswerRelevancy=\\frac{NumberRelevantStatements}{TotalStatements}\n",
    "$$\n",
    "\n",
    "Reference: [Answer Relevancy](https://deepeval.com/docs/metrics-answer-relevancy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7cb6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.models import GPTModel\n",
    "\n",
    "model = GPTModel(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    # api_key='any value',\n",
    "    default_headers={\"x-api-key\": os.getenv('API_GATEWAY_KEY')},\n",
    "    base_url='https://k7uffyg03f.execute-api.us-east-1.amazonaws.com/prod/openai/v1',\n",
    ")\n",
    "\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    include_reason=True,\n",
    "    model=model,\n",
    "    \n",
    ")\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=PROMPT.format(story=blue_cross_text),\n",
    "    actual_output=response.output_text,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd7de7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.measure(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c73c9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Score**: 1.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reason**: The score is 1.00 because the response was entirely relevant, providing a clear summary of the story while including all key characters and plot points. There were no irrelevant statements, making the summary concise and focused."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f'**Score**: {metric.score}'))\n",
    "display(Markdown(f'**Reason**: {metric.reason}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa92303",
   "metadata": {},
   "source": [
    "# Other Metrics\n",
    "\n",
    "Other useful metric functions include:\n",
    "\n",
    "+ [Faithfulness](https://deepeval.com/docs/metrics-faithfulness): evaluates whether the `actual_output` factually aligns with the contents of  `retrieval_context`. \n",
    "+ [Contextual Precision](https://deepeval.com/docs/metrics-contextual-precision): evaluates whether nodes in your `retrieval_context` that are relevant to the given input are ranked higher than irrelevant ones. \n",
    "+ [Contextual Recall](https://deepeval.com/docs/metrics-contextual-recall): evaluates the extent of which the retrieval_context aligns with the expected_output. \n",
    "+ [Contextual Relevancy](https://deepeval.com/docs/metrics-contextual-relevancy): evaluates the overall relevance of the information presented in your retrieval_context for a given input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e7b6ba",
   "metadata": {},
   "source": [
    "# G-Eval\n",
    "\n",
    "[G-Eval](https://deepeval.com/docs/metrics-llm-evals) is a framework that uses LLM-as-a-judge with chain-of-thoughts (CoT) to evaluate LLM outputs based on ANY custom criteria. The G-Eval metric is the most versatile type of metric deepeval offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e63bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"You are an helpful assistant that specializes in works of fiction.\"\n",
    "PROMPT = \"\"\"\n",
    "    Based on the story below, answer the question provided.\n",
    "    <story>\n",
    "    {story}\n",
    "    </story>\n",
    "    <question>\n",
    "    Who is the main antagonist in the story and what motivates their actions?\n",
    "    </question>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df04e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    instructions=instructions,\n",
    "    input=[\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": PROMPT.format(story=blue_cross_text)}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7833729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0eb8ad",
   "metadata": {},
   "source": [
    "## Evaluation Criteria\n",
    "\n",
    "The most straightforward way to establish a metric is by using a single criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a310a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    criteria=\"Determine whether the actual output is factually correct based on the context.\",\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0312796",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input=PROMPT.format(story=blue_cross_text),\n",
    "    actual_output=response.output_text\n",
    ")\n",
    "evaluate(test_cases=[test_case], metrics=[correctness_metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9542714",
   "metadata": {},
   "source": [
    "## Evaluation Steps \n",
    "\n",
    "G-Eval is flexible in many ways: notice that we can establish an evaluation criteria or a set of evaluation steps, that can help in guiding the model to follow specific steps to perform the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8647c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the facts in 'actual output' contradicts any facts in 'input'\",\n",
    "        \"You should also heavily penalize omission of detail\",\n",
    "        \"Vague language, or contradicting OPINIONS, are not OK\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e44fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input=PROMPT.format(story=blue_cross_text),\n",
    "    actual_output=response.output_text\n",
    ")\n",
    "result = evaluate(test_cases=[test_case], metrics=[correctness_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa52080",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.model_dump()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
